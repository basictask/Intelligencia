{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Gyakorlat: Transzformáló architektúrák\n",
    "## Könyvtárak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9045a2b667947c084d26d8486531529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Kérdés-válasz kontextus alapján"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: to land humans on the moon and ensure their safe return to earth\n"
     ]
    }
   ],
   "source": [
    "# Előretanított QA modell és tokenizáló beimportálása\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Függvény aminek segítségével kontextus alapján képes választ adni a modell\n",
    "def answer_question(question, context):\n",
    "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "\n",
    "    # A leginkább valószínű kezdete és vége a válasznak\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    return answer\n",
    "\n",
    "# Példa kontextus és kérdés\n",
    "context = \"The Apollo program was a series of space missions undertaken by NASA between 1961 and 1972. Its goal was to land humans on the Moon and ensure their safe return to Earth.\"\n",
    "question = \"What was the goal of the Apollo program?\"\n",
    "\n",
    "# Válasz\n",
    "answer = answer_question(question, context)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Jelentés elemzés\n",
    "Pozitív vagy negatív a jelentés tartalma egy mondatnak?  \n",
    "A példában egy DistilBert típusú modell került tanításra az IMDB adathalmazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 11:08:48.336491: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-03 11:08:48.542941: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-03 11:08:48.542981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-03 11:08:48.574375: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-03 11:08:48.636062: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-03 11:08:49.453270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef0ef6d4780405da20a8f2e8d11717d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7905106aa445eda0c8eda80e4d9e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be8d08d24dc46df968d5c38846aebb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to /home/daniel/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c12567e7cd4d7994a87a0d4b71fb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b716dbe70faf488d86c7be3bedb237cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025f565a014746d693f9c8ce0caf7d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eda4a6901a46e1a1b27e8021409b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /home/daniel/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065c7462ba494686b5bb8b0bd30d4eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985094fd844d4f489d2aef6df1f93845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a13ea409784bbe946c35592948c230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f52fc0afa949878ea345fdaef65107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbasictask\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniel/Documents/BGE/Intelligencia/11_transformer/code/wandb/run-20231203_110936-ucbm3qls</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/basictask/huggingface/runs/ucbm3qls' target=\"_blank\">vivid-flower-5</a></strong> to <a href='https://wandb.ai/basictask/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/basictask/huggingface' target=\"_blank\">https://wandb.ai/basictask/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/basictask/huggingface/runs/ucbm3qls' target=\"_blank\">https://wandb.ai/basictask/huggingface/runs/ucbm3qls</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d75181d47d4b3dbf34be4ff3e3495f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4689 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3188, 'learning_rate': 1.7867349114949884e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2443, 'learning_rate': 1.5734698229899766e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2289, 'learning_rate': 1.3602047344849649e-05, 'epoch': 0.96}\n",
      "{'loss': 0.1702, 'learning_rate': 1.1469396459799531e-05, 'epoch': 1.28}\n",
      "{'loss': 0.147, 'learning_rate': 9.336745574749414e-06, 'epoch': 1.6}\n",
      "{'loss': 0.1403, 'learning_rate': 7.204094689699297e-06, 'epoch': 1.92}\n",
      "{'loss': 0.1018, 'learning_rate': 5.07144380464918e-06, 'epoch': 2.24}\n",
      "{'loss': 0.0807, 'learning_rate': 2.9387929195990615e-06, 'epoch': 2.56}\n",
      "{'loss': 0.0798, 'learning_rate': 8.061420345489445e-07, 'epoch': 2.88}\n",
      "{'train_runtime': 42283.4916, 'train_samples_per_second': 1.774, 'train_steps_per_second': 0.111, 'train_loss': 0.16447305547120755, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e648f9fd06a4af3ae297fbb53808c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Adathalmaz beimportálása és előfeldolgozása\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Modell betöltése\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tanító argumentumok\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Modell tanító objektum létrehozása\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "# Modell tanítása\n",
    "trainer.train()\n",
    "\n",
    "# Modell kiértékelése\n",
    "trainer.evaluate()\n",
    "\n",
    "# Modell elmentése\n",
    "model.save_pretrained(\"./distilbert-imdb\")\n",
    "tokenizer.save_pretrained(\"./distilbert-imdb\")\n",
    "\n",
    "# Predikció a modell segítségével\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return \"Positive\" if predictions[0][1] > 0.5 else \"Negative\"\n",
    "\n",
    "# Példa predikció\n",
    "print(predict_sentiment(\"This movie was fantastic!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"I haven't felt so low in years.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"I think I am getting to know you better every day\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"I have been in a car crash today\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"This day could have gone better\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modell és tokenizáló betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './distilbert-imdb/'\n",
    "# Tokenizáló betöltése\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "# Modell betöltése\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Szöveg generálás transzformáló modellel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/daniel/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed15370cdf80491dbe0c360933bb9072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2f7f1fade847e49dc84c7ab92d9cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4698, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n",
      "{'loss': 2.63, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3990.963, 'train_samples_per_second': 0.601, 'train_steps_per_second': 0.301, 'train_loss': 2.9418087005615234, 'epoch': 3.0}\n",
      "0: The scientific study of the hymenorrhino was performed from 1791 to 1794, and was conducted as part of the Expedition to the West. The specimens were selected for examination at Fort McKay in the New Hampshire, then named as the \"\n",
      "1: The scientific study of the species, written at the University of Glasgow, shows that the most common stage is at the equatorial pole, where temperatures are between 24 and 28 ° C. For purposes of comparison, it is suggested that it is also possible\n",
      "2: The scientific study of the subject was carried out in collaboration with the Director of the Central Department of Geology and of the Department of Geodetic Sciences. The study had two important aspects : the objective was to determine the size and distribution of the deposit\n"
     ]
    }
   ],
   "source": [
    "# Könyvtárak importálása\n",
    "from transformers import Trainer\n",
    "from transformers import TextDataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Alőretanított modell és tokeniztáló\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Adathalmaz betöltése\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "\n",
    "# Az adathalmaznak egy kis része kerül felhasználásra\n",
    "n_samples = 30000\n",
    "small_dataset = dataset.select(range(min(len(dataset), n_samples))) # A range teszőlegesen állítható\n",
    "\n",
    "def encode(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "small_dataset = small_dataset.map(encode, batched=True)\n",
    "\n",
    "# Adathalmaz kettéválasztása tanító és teszt adatokra\n",
    "small_train_dataset = small_dataset.shuffle().select(range(800)) # 80% tanító\n",
    "small_eval_dataset = small_dataset.shuffle().select(range(800, 1000)) # 20% validáló\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Modell finomhangolása\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Szöveg generálása\n",
    "def gen_text(input_text, n_return_sequences=3):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_length=50, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_return_sequences=n_return_sequences\n",
    "    )\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text('We have been very lucky to')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
