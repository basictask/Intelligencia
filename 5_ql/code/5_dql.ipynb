{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Gyakorlat: Mély $Q$-tanulás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/daniel/anaconda3/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /home/daniel/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/daniel/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/daniel/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/daniel/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/daniel/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daniel/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/daniel/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Könyvtárak\n",
    "import os\n",
    "import gym\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib\n",
    "import base64, io\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from scipy.signal import fftconvolve\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Konfigurációk\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "%matplotlib inline\n",
    "matplotlib.rc('animation', html='jshtml')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segédfüggvények"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeleneti változó inicializálása\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "# Képkockák lejátszása egymás után\n",
    "def plot_animation(frames, repeat=True, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "# Mozgóátlagolás (gyors Fourier-transzformáció konvolúcióval)\n",
    "def window_avg(lst, window_size):\n",
    "    window_size = int(window_size)\n",
    "    kernel = np.ones(window_size) / window_size\n",
    "    return fftconvolve(lst, kernel, mode='valid')\n",
    "\n",
    "# Teljes átlagolás\n",
    "def rolling_avg(lst):\n",
    "    lst_sum = np.cumsum(lst)\n",
    "    lst_avg = lst_sum / (np.arange(len(lst)) + 1)\n",
    "    return list(lst_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Környezet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cselekvések tere (A): Discrete(4)\n",
      "Állapotok tere: Discrete(64)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=True, render_mode='rgb_array')\n",
    "\n",
    "print(f\"Cselekvések tere (A): {env.action_space}\")\n",
    "print(f'Állapotok tere: {env.observation_space}')\n",
    "\n",
    "# Cselekvések társítása kiíratási műveletekhez\n",
    "action_mapping = {\n",
    "    0: \"←\",\n",
    "    1: \"↓\",\n",
    "    2: \"→\",\n",
    "    3: \"↑\"\n",
    "}\n",
    "\n",
    "A = list(action_mapping.keys())  # Cselekvési tér"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## $Q$-hálózat az ügynökhöz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):  # Architektúra definíció\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)  # Input réteg (teljesen becsatolt)\n",
    "        self.fc2 = nn.Linear(64, 64)  # Rejtett réteg (teljesen becsatolt)\n",
    "        self.fc3 = nn.Linear(64, action_size)  # Output réteg (teljesen becsatolt)\n",
    "        \n",
    "    def forward(self, state):  # Előrecsatolási csővezeték\n",
    "        x = self.fc1(state)  # Első rétegen áramoltatás\n",
    "        x = F.relu(x)  # Első réteg aktivációs függvénye (ReLu)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return self.fc3(x)  # Output előállítása"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tapasztalat visszajátszás az ügynökhöz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])  # Nevesített tömb típus létrehozása\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)  # Egy nevesített tömb létrehozása a megkapott paraméterekkel\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)  # Véletlen minta a memóriából\n",
    "\n",
    "        # A véletlen minta átalakítása\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float()\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float()\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n",
    "  \n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):  # Ha a __len__ függvény definiálva van le lehet kérdezni a struktúra hosszát a len() függvénnyel\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ügynök"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, alpha, gamma, state_size, action_size, buffer_size, batch_size, update_every):\n",
    "        # Belső paraméterek\n",
    "        self.t_step = 0\n",
    "        self.alpha = alpha  # Tanulási sebesség\n",
    "        self.gamma = gamma  # Diszkont faktor\n",
    "        self.batch_size = batch_size  # Kötegméret a tanuláshoz (ekkora mintát fog venni az ügynök a tapasztalat visszajátszásból)\n",
    "        self.action_size = action_size  # Lehetséges cselekvések száma (ekkora lesz a Q-hálózat output rétege)\n",
    "        self.buffer_size = buffer_size  # Buffer méret (összesen ennyi rekord lesz a tapasztalat memóriában)\n",
    "        self.update_every = update_every  # Ennyi tanítási iterációnként fog történni egy tanítási lépés\n",
    "        # Hálózat és belső memória\n",
    "        self.qnetwork = QNetwork(state_size, action_size)  # Q-hálózat\n",
    "        self.optimizer = optim.Adam(self.qnetwork.parameters(), lr=alpha)  # Optimalizáló algoritmus\n",
    "        self.memory = ReplayBuffer(action_size, buffer_size, batch_size)  # Tapasztalat memória\n",
    "        # Követési struktúrák\n",
    "        self.mse_track = []  # MSE hiba követése\n",
    "        self.action_stack = np.zeros((0, 2))  # Cselekvések követése\n",
    "        self.state_action_stack = np.zeros((0, action_size))  # Állapotok követése\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)  # A tapasztalat hozzáadása a tapasztalati memóriához\n",
    "        self.action_stack = np.vstack([self.action_stack, np.array([action, reward])])\n",
    "        self.t_step = (self.t_step + 1) % self.update_every  # Ha elér 0-hoz akkor tanítási ciklus fog történni\n",
    "\n",
    "        if self.t_step == 0 and len(self.memory) > self.update_every:  # Ha az ügynök elért egy tanítási iterációt és van elég összegyűlt tapasztalat\n",
    "            experiences = self.memory.sample()  # A tapasztalat memória véletlen mintázása\n",
    "            self.learn(experiences, self.gamma)  # Tanulás a tapasztalattal\n",
    "\n",
    "    def act(self, state, eps=0.):  # Cselekvés választása az állapot alapján\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.qnetwork.eval()  # Hálózát átállítása kiértékelési módba\n",
    "        with torch.no_grad():  # Predikció végrehajtása\n",
    "            action_values = self.qnetwork(state)  \n",
    "        self.qnetwork.train()  # Hálózat tanító módba állítása\n",
    "\n",
    "        self.state_action_stack = np.vstack([self.state_action_stack, np.array(action_values)])  # Követési struktúrához hozzáfűzés\n",
    "\n",
    "        if(random.random() > eps):  # Epszilon-mohó cselekvés választás\n",
    "            return np.argmax(action_values.cpu().data.numpy())  # Legjobb cselekvés az aktuális állapotból\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))  # Véletlen cselekvés\n",
    "\n",
    "    def learn(self, experiences):  # Ügynök tanulásának eljárása\n",
    "        states, actions, rewards, next_states, dones = experiences  # Véletlen miniköteg kicsomagolása\n",
    "\n",
    "        # Költség kiszámítása\n",
    "        q_targets_next = self.qnetwork(next_states).detach().max(1)[0].unsqueeze(1)  # Q-értékek a következő állapotban\n",
    "        q_targets = rewards + self.gamma * q_targets_next * (1 - dones)  # Q célértékek számítása\n",
    "        q_expected = self.qnetwork(states).gather(1, actions)  # Q-értékek az aktuális állapotban\n",
    "        \n",
    "        # Költség számítása\n",
    "        loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.mse_track.append(loss)\n",
    "        self.optimizer.zero_grad()  # Gradiensek hozzájárulásának törlése a kötegelt költség számítása előtt\n",
    "        loss.backward()  # Hiba visszaáramoltatása a hálózatba\n",
    "        self.optimizer.step()  # Lépés végrehajtása az optimalizációs algoritmussal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
